# Malware / Supply-Chain Sanity Check

Repository checked: `https://github.com/or4k2l/enhanced-audio-anomaly-detection`

## Scope

A lightweight static review was performed for common malicious patterns:

- shell execution and command injection helpers (`os.system`, `subprocess`, `bash -c`)
- network download/exfiltration helpers (`requests`, `urllib`, `curl`, `wget`, sockets)
- dynamic code execution (`eval`, `exec`)
- unsafe deserialization points (`pickle.load`, `joblib.load`)

## Findings

### No obvious malware indicators found

No direct reverse-shell, downloader, persistence, credential-stealer, or obfuscated payload patterns were identified in project source files.

### Security-relevant loading behavior (expected, but trust-sensitive)

The project loads serialized model artifacts in a few places:

- `src/audio_anom/export.py` uses `pickle.load` in `load_model_package`.
- `src/audio_anom/models.py` uses `joblib.load` in model loading helpers.

These are normal for ML workflows but should only be used with **trusted artifacts** because unpickling arbitrary files can execute attacker-controlled code.

## Recommendation

- Treat `.pkl`/`.joblib` model files as executable/trusted content.
- Do not load model artifacts from untrusted sources.
- Prefer signing/checksums for distributed model bundles.

## Reproducible command used

```bash
rg -n "(os\\.system|subprocess|eval\\(|exec\\(|pickle\\.load|joblib\\.load|requests\\.|urllib|curl|wget|base64|socket|crypt|rm -rf|chmod \\+x|bash -c)" src examples scripts tests setup.py
```
